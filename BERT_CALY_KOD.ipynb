{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT CALY KOD",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNp/QzQJZ+EzWRxl5DYl/C1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e9b4173a9444e24a3d54e6cbb2b1628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05e22c84ac924c36bc0ada1614d7847f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_84fc265224874b46b55512c6cacb1af7",
              "IPY_MODEL_7aabb3aa77f14c859b0b9b28114c0e9e"
            ]
          }
        },
        "05e22c84ac924c36bc0ada1614d7847f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84fc265224874b46b55512c6cacb1af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_99b950f45b724dc6878c10793736980c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e64a996395da4569bfe8628f677467c9"
          }
        },
        "7aabb3aa77f14c859b0b9b28114c0e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f70b9c447f4e4972bfbf46bd16bb5e50",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 817kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4de10c818f641aaaae24e0854f63fe0"
          }
        },
        "99b950f45b724dc6878c10793736980c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e64a996395da4569bfe8628f677467c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f70b9c447f4e4972bfbf46bd16bb5e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4de10c818f641aaaae24e0854f63fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c68f4f5c77d445a889669c70ba12d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_954e01cc15724452870bbd3ebaef1fe8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8cc085ae35ab4af6b4b19359b4f69b1c",
              "IPY_MODEL_079cbe340e114a7494b3db09ceb07064"
            ]
          }
        },
        "954e01cc15724452870bbd3ebaef1fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cc085ae35ab4af6b4b19359b4f69b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_791c2cf6a49a467e930dcff9a870feeb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4679df25989b4f5c9ffc757899a40a0a"
          }
        },
        "079cbe340e114a7494b3db09ceb07064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a783dc6077a48c5801c1ef3ada91d64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 589B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2e39f3c1b924141b994cdfa495993bb"
          }
        },
        "791c2cf6a49a467e930dcff9a870feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4679df25989b4f5c9ffc757899a40a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a783dc6077a48c5801c1ef3ada91d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2e39f3c1b924141b994cdfa495993bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1493f2faabe8416f89e87e5a8af5e747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_389fd01fbf704d5397f0f1ea9aa31256",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f8d836b2b9e4b3ca74b63f231b58013",
              "IPY_MODEL_055735e39f8a4284af060acde5bcc826"
            ]
          }
        },
        "389fd01fbf704d5397f0f1ea9aa31256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f8d836b2b9e4b3ca74b63f231b58013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_66d46a6a18e74e35b817c9e36bab26d5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88fa9e8abff34a07acf38fa4ec7b79ca"
          }
        },
        "055735e39f8a4284af060acde5bcc826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b07ff7446d9b49c9a7202727b44c5a43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [04:55&lt;00:00, 1.49MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8c0e9a9cf364d838cb8a403c7fb12bf"
          }
        },
        "66d46a6a18e74e35b817c9e36bab26d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88fa9e8abff34a07acf38fa4ec7b79ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b07ff7446d9b49c9a7202727b44c5a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8c0e9a9cf364d838cb8a403c7fb12bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raseth/learning/blob/master/BERT_CALY_KOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_d5jBTSjwSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7fb9b642-09d9-4faf-d5c5-2e03d9e685be"
      },
      "source": [
        "# tutorial on https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"Tensorflow:\")\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "print('--------------------------------')\n",
        "print('PyTorch:')\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow:\n",
            "Found GPU at: /device:GPU:0\n",
            "--------------------------------\n",
            "PyTorch:\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4253tK9Tj0VM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "outputId": "69c22281-7f5e-42d4-88df-bcfcaf572d36"
      },
      "source": [
        "!pip install transformers\n",
        "# installs library in Colab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 5.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=c215e8e1f964b755a6cfb37ff053d7cd2feaddbe5a9f3f673b39eb85ad0468c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljdYGmsnkLLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "fbf198ea-3a8a-4b00-98fd-18e4c03b0444"
      },
      "source": [
        "!pip install wget  \n",
        "# package to download data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=6eeb88cbe1102d16003f7f61a94619d2cb9eca09413d0a5060caf483cea70f1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V5ZOWfGkNcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b1e033d-ae1e-47f4-9fc4-7fd6d0dd5e44"
      },
      "source": [
        "# downloads dataset named CoLA - https://nyu-mll.github.io/CoLA/\n",
        "\n",
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peCaEl27kPd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "378cc24d-0896-4d1b-c29c-05769622a78d"
      },
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al7wSttDkR5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "ef9e9229-e7f7-41bb-b8ec-2d516f8d8389"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6507</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Almost every cat likes mice, but Felix doesn't.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8542</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jason persuaded Medea to desert her family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6827</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>These fields were marched over by all the armi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6354</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You may pick any flower.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6727</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I love the food they cook in the halls of resi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sharks seem to swim slowly in the tropics.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>She did away with her father.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Water poured onto the plants.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This is the book of which Bill approves, and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5823</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The building's roof is leaking.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "6507            d_98  ...    Almost every cat likes mice, but Felix doesn't.\n",
              "8542            ad03  ...         Jason persuaded Medea to desert her family\n",
              "6827            m_02  ...  These fields were marched over by all the armi...\n",
              "6354            d_98  ...                           You may pick any flower.\n",
              "6727            m_02  ...  I love the food they cook in the halls of resi...\n",
              "7622           sks13  ...         Sharks seem to swim slowly in the tropics.\n",
              "1457            r-67  ...                      She did away with her father.\n",
              "2582            l-93  ...                      Water poured onto the plants.\n",
              "911             bc01  ...  This is the book of which Bill approves, and t...\n",
              "5823            c_13  ...                    The building's roof is leaking.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VUpxuHokT_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2573004f-5925-484a-c484-d1cd5ecd3375"
      },
      "source": [
        "# we are interested in these two columns\n",
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5556</th>\n",
              "      <td>Some of them made as many as Joan errors.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1590</th>\n",
              "      <td>If it, I've lost $500.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2720</th>\n",
              "      <td>They lent me with a bicycle.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6568</th>\n",
              "      <td>Where and when did Bill put the book?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3357</th>\n",
              "      <td>The motorist happened the accident.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       sentence  label\n",
              "5556  Some of them made as many as Joan errors.      0\n",
              "1590                     If it, I've lost $500.      0\n",
              "2720               They lent me with a bicycle.      0\n",
              "6568      Where and when did Bill put the book?      0\n",
              "3357        The motorist happened the accident.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Tu7YK9kXDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to numpy\n",
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values  # both 0s and 1s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmG0yQr6kYol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98f971d3-2a9c-4479-8abf-b118cfbe22a7"
      },
      "source": [
        "print(labels[:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK41jWtqkjCi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "1e9b4173a9444e24a3d54e6cbb2b1628",
            "05e22c84ac924c36bc0ada1614d7847f",
            "84fc265224874b46b55512c6cacb1af7",
            "7aabb3aa77f14c859b0b9b28114c0e9e",
            "99b950f45b724dc6878c10793736980c",
            "e64a996395da4569bfe8628f677467c9",
            "f70b9c447f4e4972bfbf46bd16bb5e50",
            "c4de10c818f641aaaae24e0854f63fe0"
          ]
        },
        "outputId": "5436becf-13e6-41cf-c030-f96d1cc249b1"
      },
      "source": [
        "# USING BERT - tokenize sentences to bert tokens\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e9b4173a9444e24a3d54e6cbb2b1628",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvUwB1Jkkai1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b6c48ba9-e6ec-478b-bc16-b469d57d66c4"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
        "\n",
        "\n",
        "print('Both with one function: ', tokenizer.encode(sentences[0], add_special_tokens=False))\n",
        "print('Both with one function and tags like [cls], [sep]: ', tokenizer.encode(sentences[0], add_special_tokens=True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n",
            "Both with one function:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n",
            "Both with one function and tags like [cls], [sep]:  [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXQtT9kUkdE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [cls] - bertowy, odpowiednik doc embeddinga - do każdego zdania dopisujemy na początku wyraz [cls] i word embedding tego znaczka \n",
        "# potraktujemy trochę jako doc emb, ale wyciągając go z końca sieci, a nie z pierwszej warstwy \n",
        "# (w zasadzie to chyba bardziej jest to niczym komórka pamięci A z LSTM)\n",
        "# ponadto kazde zdanie musi konczyc sie na [sep]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zlvBTIokn89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ce817c5-67c1-4917-b9d3-2f99cf755379"
      },
      "source": [
        "# BERT has two constraints:\n",
        "\n",
        "# All sentences must be padded or truncated to a single, fixed length.\n",
        "# The maximum sentence length is 512 tokens.\n",
        "\n",
        "# The “Attention Mask” is simply an array of 1s and 0s indicating which tokens are padding and which aren’t\n",
        "\n",
        "# we need to decide on a maximum sentence length for padding / truncating to.\n",
        "# The below cell will perform one tokenization pass of the dataset in order to measure the maximum sentence length.\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "print(input_ids)\n",
        "\n",
        "# ostatecznie i tak wybierzemy 64, tak w razie czego, ale wiemy ze zdania sa krotkie do 50 slow"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n",
            "[101, 2054, 2035, 2106, 2017, 2131, 2005, 4234, 1029, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37w0_Tykp5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "867bc68f-66b6-4e21-c317-3e5778a95682"
      },
      "source": [
        "# co to attention mask -> https://huggingface.co/transformers/glossary.html#attention-mask\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Attention mask: ', attention_masks[0])\n",
        "\n",
        "print('Label: ', labels[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n",
            "Attention mask:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label:  tensor(1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1G4IyvErhBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a39dd2d0-c1d2-4d59-a322-c6961e5cdad8"
      },
      "source": [
        "# TRAIN AND DEV (VALIDATION) SPLIT (test bedzie pozniej)- random select 9 to 1 ratio\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAD_KHZRtZkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iterator for our dataset using the torch DataLoader class - w skrocie to generator, tak jak ten z kerasa\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyaAKsdUunQS",
        "colab_type": "text"
      },
      "source": [
        "## **4. Train Our Classification Model**\n",
        "\n",
        "**4.1. BertForSequenceClassification**\n",
        "\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
        "\n",
        "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "\n",
        "Here is the current list of classes provided for fine-tuning:\n",
        "\n",
        "    BertModel\n",
        "    BertForPreTraining\n",
        "    BertForMaskedLM\n",
        "    BertForNextSentencePrediction\n",
        "    BertForSequenceClassification - The one we’ll use.\n",
        "    BertForTokenClassification\n",
        "    BertForQuestionAnswering\n",
        "\n",
        "We’ll be using BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "OK, let’s load BERT! There are a few different pre-trained BERT models available. “bert-base-uncased” means the version that has only lowercase letters (“uncased”) and is the smaller version of the two (“base” vs “large”)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDjyKy3OurOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c68f4f5c77d445a889669c70ba12d48",
            "954e01cc15724452870bbd3ebaef1fe8",
            "8cc085ae35ab4af6b4b19359b4f69b1c",
            "079cbe340e114a7494b3db09ceb07064",
            "791c2cf6a49a467e930dcff9a870feeb",
            "4679df25989b4f5c9ffc757899a40a0a",
            "9a783dc6077a48c5801c1ef3ada91d64",
            "a2e39f3c1b924141b994cdfa495993bb",
            "1493f2faabe8416f89e87e5a8af5e747",
            "389fd01fbf704d5397f0f1ea9aa31256",
            "7f8d836b2b9e4b3ca74b63f231b58013",
            "055735e39f8a4284af060acde5bcc826",
            "66d46a6a18e74e35b817c9e36bab26d5",
            "88fa9e8abff34a07acf38fa4ec7b79ca",
            "b07ff7446d9b49c9a7202727b44c5a43",
            "d8c0e9a9cf364d838cb8a403c7fb12bf"
          ]
        },
        "outputId": "7691ef20-c986-4480-d644-0609e712ed39"
      },
      "source": [
        "# zaciagniecie i wczytanie gotowca z netu - tak jak incepcjaV3 - jeszcze bez trenowania\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c68f4f5c77d445a889669c70ba12d48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1493f2faabe8416f89e87e5a8af5e747",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyJS2kaMvSoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "b482ef12-1f35-433d-f4d6-adeec53fff86"
      },
      "source": [
        "# WYPISZ PARAMETRY\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Potem są jeszcze inne warstwy transformerowe takie jak ta duża składająca się z wielu powyżej... ====\\n')\n",
        "for p in params[21:31]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== I tak dalej z warstwami... ====\\n')\n",
        "\n",
        "print('\\n==== I na końcu Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Potem są jeszcze inne warstwy transformerowe takie jak ta duża składająca się z wielu powyżej... ====\n",
            "\n",
            "bert.encoder.layer.1.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.1.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.1.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.1.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.1.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.1.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.1.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.1.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias          (768,)\n",
            "\n",
            "==== I tak dalej z warstwami... ====\n",
            "\n",
            "\n",
            "==== I na końcu Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtrUpmjAw_QF",
        "colab_type": "text"
      },
      "source": [
        "For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n",
        "\n",
        "        Batch size: 16, 32\n",
        "        Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "        Number of epochs: 2, 3, 4\n",
        "\n",
        "We chose:\n",
        "\n",
        "    Batch size: 32 (set when creating our DataLoaders)\n",
        "    Learning rate: 2e-5\n",
        "    Epochs: 4 (we’ll see that this is probably too many…)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIbby0-w_t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKXuvTOlxIWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyrMEFbBxUtX",
        "colab_type": "text"
      },
      "source": [
        "**4.3. Training Loop**\n",
        "\n",
        "Below is our training loop. There’s a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase.\n",
        "\n",
        "Thank you to Stas Bekman for contributing the insights and code for using validation loss to detect over-fitting!\n",
        "\n",
        "Training:\n",
        "\n",
        "    Unpack our data inputs and labels\n",
        "    Load data onto the GPU for acceleration\n",
        "    Clear out the gradients calculated in the previous pass.\n",
        "        In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
        "    Forward pass (feed input data through the network)\n",
        "    Backward pass (backpropagation)\n",
        "    Tell the network to update parameters with optimizer.step()\n",
        "    Track variables for monitoring progress\n",
        "\n",
        "Evalution:\n",
        "\n",
        "    Unpack our data inputs and labels\n",
        "    Load data onto the GPU for acceleration\n",
        "    Forward pass (feed input data through the network)\n",
        "    Compute loss on our validation data and track variables for monitoring progress\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqllRoOexMvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a helper function for calculating accuracy.\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E-2YrnUxoSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function for formatting elapsed times as hh:mm:ss\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RGer-gPxsy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a14300fe-d86b-4a56-881a-235517c46dd5"
      },
      "source": [
        "# RUNNING TRAINING\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:25.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.47\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.49\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:08.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:16.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:24.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:41.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:49.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:00:49\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.56\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:03:23 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQoekmKy763",
        "colab_type": "text"
      },
      "source": [
        "## PODSUMOWANIE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBrEJAfgy8Kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fcb11f78-ac38-430a-f496-0262d92ebafc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.48</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0:00:49</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:00:49</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0:00:49</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:00:49</td>\n",
              "      <td>0:00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.48         0.47           0.80       0:00:49         0:00:02\n",
              "2               0.30         0.44           0.83       0:00:49         0:00:02\n",
              "3               0.20         0.49           0.82       0:00:49         0:00:02\n",
              "4               0.14         0.56           0.84       0:00:49         0:00:02"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8-n9UezLLA",
        "colab_type": "text"
      },
      "source": [
        "Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it’s over-fitting on the training data.\n",
        "\n",
        "(For reference, we are using 7,695 training samples and 856 validation samples).\n",
        "\n",
        "Validation Loss is a more precise measure than accuracy, because with accuracy we don’t care about the exact output value, but just which side of a threshold it falls on.\n",
        "\n",
        "If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS8YefMOzLYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "5a90dad6-56f4-4da5-b3ef-520a53f60d05"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qTfS6ySzTXx",
        "colab_type": "text"
      },
      "source": [
        "# **5. Performance On Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpbqhEzRzVu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6d4c1f8c-be31-42b0-9386-6205a47f1071"
      },
      "source": [
        "# 5.1. Data Preparation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQsapajszel0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "912d2a38-0fb1-4bdc-9a5b-8bffc0b69533"
      },
      "source": [
        "# 5.2. Evaluate on Test Set - PREDICT\n",
        "\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIkLHDKzsXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53c6af33-4ea5-405c-f872-8395c2497621"
      },
      "source": [
        "# Accuracy on the CoLA benchmark is measured using the “Matthews correlation coefficient” (MCC).\n",
        "# We use MCC here because the classes are IMBALANCED:\n",
        "\n",
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7xSMAhnz6CB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3ece9bd4-b87a-47fb-b0d1-707520f56131"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LrSe0wKz9q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "f624d6c1-6b77-48c4-e04d-ff3200a30bbb"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQHOquZX0BNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "484851f6-7e88-49ed-a6e2-377d3a898a67"
      },
      "source": [
        "# Now we’ll combine the results for all of the batches and calculate our final MCC score.\n",
        "\n",
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total MCC: 0.540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFh7gv1J0SbD",
        "colab_type": "text"
      },
      "source": [
        "# Appendix\n",
        "# **A1. Saving & Loading Fine-Tuned Model**\n",
        "This first cell (taken from run_glue.py here) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNClb2xb0SkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "6acb698d-f554-4fe2-c0b1-79a7b4c46cbb"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCPT6k4T0aos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "be56082d-3c39-46aa-a7a3-e72df992593c"
      },
      "source": [
        "# check model file size\n",
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427960K\n",
            "-rw-r--r-- 1 root root      2K May  1 15:15 config.json\n",
            "-rw-r--r-- 1 root root 427719K May  1 15:15 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K May  1 15:15 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K May  1 15:15 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K May  1 15:15 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm-ttBh20ddl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "58c42fb2-a76b-4352-f318-52af28147652"
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M May  1 15:15 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmuTtuuk0lhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive.\n",
        "\n",
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toxzo6pP0x3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4dUDGRU0yXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following functions will load the model back from disk.\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwh0dyIQ1CyP",
        "colab_type": "text"
      },
      "source": [
        "## **A.2. Weight Decay**\n",
        "The huggingface example includes the following code block for enabling weight decay, but the default decay rate is “0.0”, so I moved this to the appendix.\n",
        "\n",
        "This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization–after calculating the gradients, we multiply them by, e.g., 0.99."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oklHQFbM1C5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "095d8474-7132-4982-9283-001c45e8d450"
      },
      "source": [
        "# This code is taken from:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102\n",
        "\n",
        "# Don't apply weight decay to any parameters whose names include these tokens.\n",
        "# (Here, the BERT doesn't have `gamma` or `beta` parameters, only `bias` terms)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "# Separate the `weight` parameters from the `bias` parameters. \n",
        "# - For the `weight` parameters, this specifies a 'weight_decay_rate' of 0.01. \n",
        "# - For the `bias` parameters, the 'weight_decay_rate' is 0.0. \n",
        "optimizer_grouped_parameters = [\n",
        "    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.1},\n",
        "    \n",
        "    # Filter for parameters which *do* include those.\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Note - `optimizer_grouped_parameters` only includes the parameter values, not \n",
        "# the names."
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3e40b4963317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer_grouped_parameters = [\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n\u001b[0m\u001b[1;32m      9\u001b[0m      'weight_decay_rate': 0.1},\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'param_optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu0ZIPhT1MKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}